{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üè¶ Bank Fraud Detection - ML Training Pipeline\n",
                "\n",
                "## Overview\n",
                "This notebook implements a **professional fraud detection ML pipeline** with:\n",
                "- **Dataset**: 50,000 realistic banking transactions\n",
                "- **Features**: Real, interpretable features (Amount, Location, Channel, Age, Occupation, etc.)\n",
                "- **Models**: Random Forest, XGBoost, LightGBM, CatBoost\n",
                "- **Optimization**: Optuna hyperparameter tuning\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# ML imports\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import (\n",
                "    classification_report, confusion_matrix, roc_auc_score, \n",
                "    precision_recall_curve, roc_curve, f1_score, precision_score,\n",
                "    recall_score, average_precision_score, accuracy_score\n",
                ")\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "# Boosting models\n",
                "import xgboost as xgb\n",
                "import lightgbm as lgb\n",
                "from catboost import CatBoostClassifier\n",
                "\n",
                "# Hyperparameter optimization\n",
                "import optuna\n",
                "from optuna.samplers import TPESampler\n",
                "\n",
                "# Imbalanced data handling\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "# Model persistence\n",
                "import joblib\n",
                "import json\n",
                "import os\n",
                "from datetime import datetime\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print('‚úÖ All imports successful!')\n",
                "print(f'üìÖ Notebook run: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the synthetic dataset with real features\n",
                "DATA_PATH = 'data/fraud_dataset.csv'\n",
                "\n",
                "if not os.path.exists(DATA_PATH):\n",
                "    print('‚ö†Ô∏è Dataset not found. Generating...')\n",
                "    exec(open('generate_dataset.py').read())\n",
                "\n",
                "df = pd.read_csv(DATA_PATH)\n",
                "print(f'‚úÖ Dataset loaded!')\n",
                "print(f'üìä Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset overview\n",
                "print('=' * 60)\n",
                "print('üìã DATASET OVERVIEW')\n",
                "print('=' * 60)\n",
                "print(f'\\nüî¢ Total Transactions: {len(df):,}')\n",
                "print(f'üìÅ Features: {df.shape[1]}')\n",
                "\n",
                "# Class distribution\n",
                "fraud_count = df['is_fraud'].sum()\n",
                "normal_count = len(df) - fraud_count\n",
                "fraud_pct = (fraud_count / len(df)) * 100\n",
                "\n",
                "print(f'\\nüìä Class Distribution:')\n",
                "print(f'   ‚úÖ Normal: {normal_count:,} ({100-fraud_pct:.2f}%)')\n",
                "print(f'   üö® Fraud:  {fraud_count:,} ({fraud_pct:.2f}%)')\n",
                "print(f'\\n‚öñÔ∏è Imbalance Ratio: 1:{int(normal_count/fraud_count)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature overview\n",
                "print('üìã Feature Types:')\n",
                "print(df.dtypes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize class distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "colors = ['#2ecc71', '#e74c3c']\n",
                "axes[0].pie([normal_count, fraud_count], labels=['Normal', 'Fraud'], \n",
                "            autopct='%1.2f%%', colors=colors, explode=(0, 0.1), shadow=True)\n",
                "axes[0].set_title('Transaction Class Distribution', fontsize=14, fontweight='bold')\n",
                "\n",
                "bars = axes[1].bar(['Normal', 'Fraud'], [normal_count, fraud_count], color=colors, edgecolor='black')\n",
                "axes[1].set_ylabel('Count', fontsize=12)\n",
                "axes[1].set_title('Transaction Counts by Class', fontsize=14, fontweight='bold')\n",
                "for bar, count in zip(bars, [normal_count, fraud_count]):\n",
                "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
                "                f'{count:,}', ha='center', fontsize=11, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transaction Amount Distribution by Fraud Status\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# Amount by fraud status\n",
                "df.boxplot(column='TransactionAmount', by='is_fraud', ax=axes[0])\n",
                "axes[0].set_xlabel('Fraud (0=No, 1=Yes)')\n",
                "axes[0].set_ylabel('Amount ($)')\n",
                "axes[0].set_title('Transaction Amount by Fraud Status', fontweight='bold')\n",
                "plt.suptitle('')\n",
                "\n",
                "# Hour distribution\n",
                "df[df['is_fraud']==0]['hour'].hist(ax=axes[1], bins=24, alpha=0.7, label='Normal', color='green')\n",
                "df[df['is_fraud']==1]['hour'].hist(ax=axes[1], bins=24, alpha=0.7, label='Fraud', color='red')\n",
                "axes[1].set_xlabel('Hour of Day')\n",
                "axes[1].set_ylabel('Frequency')\n",
                "axes[1].set_title('Transaction Hour Distribution', fontweight='bold')\n",
                "axes[1].legend()\n",
                "\n",
                "# Login attempts\n",
                "df[df['is_fraud']==0]['LoginAttempts'].value_counts().sort_index().plot(kind='bar', ax=axes[2], alpha=0.7, label='Normal', color='green', position=1, width=0.4)\n",
                "df[df['is_fraud']==1]['LoginAttempts'].value_counts().sort_index().plot(kind='bar', ax=axes[2], alpha=0.7, label='Fraud', color='red', position=0, width=0.4)\n",
                "axes[2].set_xlabel('Login Attempts')\n",
                "axes[2].set_ylabel('Frequency')\n",
                "axes[2].set_title('Login Attempts by Fraud Status', fontweight='bold')\n",
                "axes[2].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Categorical features analysis\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "\n",
                "# Transaction Type\n",
                "pd.crosstab(df['TransactionType'], df['is_fraud'], normalize='index').plot(kind='bar', ax=axes[0,0], color=['green', 'red'])\n",
                "axes[0,0].set_title('Fraud Rate by Transaction Type', fontweight='bold')\n",
                "axes[0,0].set_ylabel('Proportion')\n",
                "axes[0,0].legend(['Normal', 'Fraud'])\n",
                "\n",
                "# Channel\n",
                "pd.crosstab(df['Channel'], df['is_fraud'], normalize='index').plot(kind='bar', ax=axes[0,1], color=['green', 'red'])\n",
                "axes[0,1].set_title('Fraud Rate by Channel', fontweight='bold')\n",
                "axes[0,1].set_ylabel('Proportion')\n",
                "axes[0,1].legend(['Normal', 'Fraud'])\n",
                "\n",
                "# Occupation\n",
                "pd.crosstab(df['CustomerOccupation'], df['is_fraud'], normalize='index').plot(kind='bar', ax=axes[1,0], color=['green', 'red'])\n",
                "axes[1,0].set_title('Fraud Rate by Occupation', fontweight='bold')\n",
                "axes[1,0].set_ylabel('Proportion')\n",
                "axes[1,0].legend(['Normal', 'Fraud'])\n",
                "axes[1,0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# Merchant Category\n",
                "pd.crosstab(df['MerchantCategory'], df['is_fraud'], normalize='index').plot(kind='bar', ax=axes[1,1], color=['green', 'red'])\n",
                "axes[1,1].set_title('Fraud Rate by Merchant Category', fontweight='bold')\n",
                "axes[1,1].set_ylabel('Proportion')\n",
                "axes[1,1].legend(['Normal', 'Fraud'])\n",
                "axes[1,1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features\n",
                "# Drop non-predictive columns\n",
                "drop_cols = ['TransactionID', 'AccountID', 'TransactionDate']\n",
                "df_model = df.drop(columns=drop_cols)\n",
                "\n",
                "# Encode categorical variables\n",
                "categorical_cols = ['TransactionType', 'Location', 'Channel', 'MerchantCategory', 'CustomerOccupation']\n",
                "label_encoders = {}\n",
                "\n",
                "for col in categorical_cols:\n",
                "    le = LabelEncoder()\n",
                "    df_model[col] = le.fit_transform(df_model[col])\n",
                "    label_encoders[col] = le\n",
                "\n",
                "print('‚úÖ Categorical variables encoded!')\n",
                "print(f'üìä Features shape: {df_model.shape}')\n",
                "df_model.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split features and target\n",
                "X = df_model.drop('is_fraud', axis=1)\n",
                "y = df_model['is_fraud']\n",
                "\n",
                "feature_names = list(X.columns)\n",
                "print(f'üìã Features: {feature_names}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale numerical features\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "X_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n",
                "\n",
                "print('‚úÖ Features scaled!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train/Validation/Test Split (70/15/15)\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(\n",
                "    X_scaled, y, test_size=0.30, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "X_val, X_test, y_val, y_test = train_test_split(\n",
                "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
                ")\n",
                "\n",
                "print('üìä Data Split Summary')\n",
                "print('=' * 50)\n",
                "print(f'Training:   {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)')\n",
                "print(f'Validation: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)')\n",
                "print(f'Test:       {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)')\n",
                "print(f'\\nüéØ Fraud ratios preserved:')\n",
                "print(f'   Train: {y_train.mean()*100:.2f}%')\n",
                "print(f'   Val:   {y_val.mean()*100:.2f}%')\n",
                "print(f'   Test:  {y_test.mean()*100:.2f}%')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply SMOTE to handle class imbalance\n",
                "print('‚öñÔ∏è Applying SMOTE...')\n",
                "smote = SMOTE(random_state=42, sampling_strategy=0.5)\n",
                "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
                "\n",
                "print(f'\\nüìä Before SMOTE: Normal={sum(y_train==0):,}, Fraud={sum(y_train==1):,}')\n",
                "print(f'üìä After SMOTE:  Normal={sum(y_train_resampled==0):,}, Fraud={sum(y_train_resampled==1):,}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Training with Optuna"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Results storage\n",
                "results = {}\n",
                "\n",
                "def evaluate_model(model, X_test, y_test, model_name):\n",
                "    \"\"\"Evaluate model and return metrics\"\"\"\n",
                "    y_pred = model.predict(X_test)\n",
                "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_test, y_pred),\n",
                "        'precision': precision_score(y_test, y_pred),\n",
                "        'recall': recall_score(y_test, y_pred),\n",
                "        'f1': f1_score(y_test, y_pred),\n",
                "        'auc_roc': roc_auc_score(y_test, y_pred_proba),\n",
                "        'avg_precision': average_precision_score(y_test, y_pred_proba)\n",
                "    }\n",
                "    \n",
                "    print(f'\\nüéØ {model_name} Performance:')\n",
                "    print('=' * 50)\n",
                "    for name, value in metrics.items():\n",
                "        print(f\"   {name}: {value:.4f}\")\n",
                "    \n",
                "    return metrics, y_pred, y_pred_proba"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optuna config\n",
                "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
                "sampler = TPESampler(seed=42)\n",
                "N_TRIALS = 20  # Adjust for more thorough search"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Forest\n",
                "print('üå≤ Training Random Forest...')\n",
                "\n",
                "def objective_rf(trial):\n",
                "    params = {\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
                "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
                "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
                "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
                "        'class_weight': 'balanced',\n",
                "        'random_state': 42,\n",
                "        'n_jobs': -1\n",
                "    }\n",
                "    model = RandomForestClassifier(**params)\n",
                "    model.fit(X_train_resampled, y_train_resampled)\n",
                "    return roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
                "\n",
                "study_rf = optuna.create_study(direction='maximize', sampler=sampler)\n",
                "study_rf.optimize(objective_rf, n_trials=N_TRIALS, show_progress_bar=True)\n",
                "\n",
                "# Train best RF\n",
                "best_rf_params = study_rf.best_trial.params\n",
                "best_rf_params.update({'class_weight': 'balanced', 'random_state': 42, 'n_jobs': -1})\n",
                "rf_model = RandomForestClassifier(**best_rf_params)\n",
                "rf_model.fit(X_train_resampled, y_train_resampled)\n",
                "results['Random Forest'], rf_pred, rf_proba = evaluate_model(rf_model, X_test, y_test, 'Random Forest')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XGBoost\n",
                "print('üöÄ Training XGBoost...')\n",
                "\n",
                "def objective_xgb(trial):\n",
                "    params = {\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
                "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
                "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
                "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
                "        'scale_pos_weight': sum(y_train==0)/sum(y_train==1),\n",
                "        'random_state': 42,\n",
                "        'eval_metric': 'auc'\n",
                "    }\n",
                "    model = xgb.XGBClassifier(**params)\n",
                "    model.fit(X_train_resampled, y_train_resampled, verbose=False)\n",
                "    return roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
                "\n",
                "study_xgb = optuna.create_study(direction='maximize', sampler=sampler)\n",
                "study_xgb.optimize(objective_xgb, n_trials=N_TRIALS, show_progress_bar=True)\n",
                "\n",
                "# Train best XGB\n",
                "best_xgb_params = study_xgb.best_trial.params\n",
                "best_xgb_params.update({'scale_pos_weight': sum(y_train==0)/sum(y_train==1), 'random_state': 42, 'eval_metric': 'auc'})\n",
                "xgb_model = xgb.XGBClassifier(**best_xgb_params)\n",
                "xgb_model.fit(X_train_resampled, y_train_resampled, verbose=False)\n",
                "results['XGBoost'], xgb_pred, xgb_proba = evaluate_model(xgb_model, X_test, y_test, 'XGBoost')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LightGBM\n",
                "print('‚ö° Training LightGBM...')\n",
                "\n",
                "def objective_lgb(trial):\n",
                "    params = {\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
                "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
                "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
                "        'class_weight': 'balanced',\n",
                "        'random_state': 42,\n",
                "        'verbose': -1\n",
                "    }\n",
                "    model = lgb.LGBMClassifier(**params)\n",
                "    model.fit(X_train_resampled, y_train_resampled)\n",
                "    return roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
                "\n",
                "study_lgb = optuna.create_study(direction='maximize', sampler=sampler)\n",
                "study_lgb.optimize(objective_lgb, n_trials=N_TRIALS, show_progress_bar=True)\n",
                "\n",
                "# Train best LGB\n",
                "best_lgb_params = study_lgb.best_trial.params\n",
                "best_lgb_params.update({'class_weight': 'balanced', 'random_state': 42, 'verbose': -1})\n",
                "lgb_model = lgb.LGBMClassifier(**best_lgb_params)\n",
                "lgb_model.fit(X_train_resampled, y_train_resampled)\n",
                "results['LightGBM'], lgb_pred, lgb_proba = evaluate_model(lgb_model, X_test, y_test, 'LightGBM')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CatBoost\n",
                "print('üê± Training CatBoost...')\n",
                "\n",
                "def objective_cat(trial):\n",
                "    params = {\n",
                "        'iterations': trial.suggest_int('iterations', 100, 300),\n",
                "        'depth': trial.suggest_int('depth', 4, 8),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
                "        'auto_class_weights': 'Balanced',\n",
                "        'random_state': 42,\n",
                "        'verbose': False\n",
                "    }\n",
                "    model = CatBoostClassifier(**params)\n",
                "    model.fit(X_train_resampled, y_train_resampled, verbose=False)\n",
                "    return roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
                "\n",
                "study_cat = optuna.create_study(direction='maximize', sampler=sampler)\n",
                "study_cat.optimize(objective_cat, n_trials=N_TRIALS, show_progress_bar=True)\n",
                "\n",
                "# Train best CatBoost\n",
                "best_cat_params = study_cat.best_trial.params\n",
                "best_cat_params.update({'auto_class_weights': 'Balanced', 'random_state': 42, 'verbose': False})\n",
                "cat_model = CatBoostClassifier(**best_cat_params)\n",
                "cat_model.fit(X_train_resampled, y_train_resampled, verbose=False)\n",
                "results['CatBoost'], cat_pred, cat_proba = evaluate_model(cat_model, X_test, y_test, 'CatBoost')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare all models\n",
                "comparison_df = pd.DataFrame(results).T.round(4)\n",
                "comparison_df = comparison_df.sort_values('auc_roc', ascending=False)\n",
                "\n",
                "print('\\nüèÜ MODEL COMPARISON (sorted by AUC-ROC)')\n",
                "print('=' * 80)\n",
                "print(comparison_df.to_string())\n",
                "\n",
                "best_model_name = comparison_df.index[0]\n",
                "print(f'\\nü•á Best Model: {best_model_name}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "\n",
                "# Metrics comparison\n",
                "x = np.arange(len(comparison_df.index))\n",
                "width = 0.2\n",
                "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
                "for i, metric in enumerate(['auc_roc', 'precision', 'recall', 'f1']):\n",
                "    axes[0, 0].bar(x + i*width, comparison_df[metric], width, label=metric.upper(), color=colors[i])\n",
                "axes[0, 0].set_xticks(x + width*1.5)\n",
                "axes[0, 0].set_xticklabels(comparison_df.index, rotation=15)\n",
                "axes[0, 0].legend()\n",
                "axes[0, 0].set_title('Model Comparison', fontweight='bold')\n",
                "axes[0, 0].set_ylim(0, 1.05)\n",
                "\n",
                "# ROC Curves\n",
                "models = {'Random Forest': rf_proba, 'XGBoost': xgb_proba, 'LightGBM': lgb_proba, 'CatBoost': cat_proba}\n",
                "for name, proba in models.items():\n",
                "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
                "    auc = roc_auc_score(y_test, proba)\n",
                "    axes[0, 1].plot(fpr, tpr, label=f'{name} (AUC={auc:.4f})', linewidth=2)\n",
                "axes[0, 1].plot([0, 1], [0, 1], 'k--')\n",
                "axes[0, 1].set_xlabel('False Positive Rate')\n",
                "axes[0, 1].set_ylabel('True Positive Rate')\n",
                "axes[0, 1].set_title('ROC Curves', fontweight='bold')\n",
                "axes[0, 1].legend()\n",
                "\n",
                "# Precision-Recall Curves\n",
                "for name, proba in models.items():\n",
                "    precision, recall, _ = precision_recall_curve(y_test, proba)\n",
                "    ap = average_precision_score(y_test, proba)\n",
                "    axes[1, 0].plot(recall, precision, label=f'{name} (AP={ap:.4f})', linewidth=2)\n",
                "axes[1, 0].set_xlabel('Recall')\n",
                "axes[1, 0].set_ylabel('Precision')\n",
                "axes[1, 0].set_title('Precision-Recall Curves', fontweight='bold')\n",
                "axes[1, 0].legend()\n",
                "\n",
                "# Confusion Matrix for best model\n",
                "best_pred = {'Random Forest': rf_pred, 'XGBoost': xgb_pred, 'LightGBM': lgb_pred, 'CatBoost': cat_pred}[best_model_name]\n",
                "cm = confusion_matrix(y_test, best_pred)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1],\n",
                "            xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])\n",
                "axes[1, 1].set_xlabel('Predicted')\n",
                "axes[1, 1].set_ylabel('Actual')\n",
                "axes[1, 1].set_title(f'Confusion Matrix - {best_model_name}', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('notebooks/model_comparison.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Importance (XGBoost)\n",
                "importance = pd.DataFrame({\n",
                "    'feature': feature_names,\n",
                "    'importance': xgb_model.feature_importances_\n",
                "}).sort_values('importance', ascending=True)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.barh(importance['feature'], importance['importance'], color='#8b5cf6')\n",
                "plt.xlabel('Importance')\n",
                "plt.title('Feature Importance (XGBoost)', fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('notebooks/feature_importance.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create models directory\n",
                "os.makedirs('models', exist_ok=True)\n",
                "\n",
                "# Save all models\n",
                "models_to_save = {\n",
                "    'random_forest': rf_model,\n",
                "    'xgboost': xgb_model,\n",
                "    'lightgbm': lgb_model,\n",
                "    'catboost': cat_model\n",
                "}\n",
                "\n",
                "for name, model in models_to_save.items():\n",
                "    joblib.dump(model, f'models/{name}_model.pkl')\n",
                "    print(f'‚úÖ Saved: models/{name}_model.pkl')\n",
                "\n",
                "# Save best model\n",
                "best_model = {'Random Forest': rf_model, 'XGBoost': xgb_model, 'LightGBM': lgb_model, 'CatBoost': cat_model}[best_model_name]\n",
                "joblib.dump(best_model, 'models/best_model.pkl')\n",
                "print(f'\\nüèÜ Best model saved as: models/best_model.pkl')\n",
                "\n",
                "# Save scaler and encoders\n",
                "joblib.dump(scaler, 'models/scaler.pkl')\n",
                "joblib.dump(label_encoders, 'models/label_encoders.pkl')\n",
                "print('‚úÖ Saved: models/scaler.pkl')\n",
                "print('‚úÖ Saved: models/label_encoders.pkl')\n",
                "\n",
                "# Save feature names\n",
                "with open('models/feature_names.json', 'w') as f:\n",
                "    json.dump(feature_names, f)\n",
                "print('‚úÖ Saved: models/feature_names.json')\n",
                "\n",
                "# Save metrics\n",
                "metrics_dict = {\n",
                "    'best_model': best_model_name,\n",
                "    'results': {k: {m: float(v) for m, v in metrics.items()} for k, metrics in results.items()},\n",
                "    'training_date': datetime.now().isoformat(),\n",
                "    'dataset_size': len(df),\n",
                "    'fraud_rate': float(fraud_pct)\n",
                "}\n",
                "\n",
                "with open('models/model_metrics.json', 'w') as f:\n",
                "    json.dump(metrics_dict, f, indent=2)\n",
                "print('‚úÖ Saved: models/model_metrics.json')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('\\n' + '=' * 70)\n",
                "print('üéâ TRAINING COMPLETE')\n",
                "print('=' * 70)\n",
                "\n",
                "print(f'\\nüìä Dataset: {len(df):,} transactions')\n",
                "print(f'   Fraud rate: {fraud_pct:.2f}%')\n",
                "print(f'   Features: {len(feature_names)}')\n",
                "\n",
                "print(f'\\nü§ñ Models Trained:')\n",
                "for name, metrics in results.items():\n",
                "    print(f'   - {name}: AUC={metrics[\"auc_roc\"]:.4f}, F1={metrics[\"f1\"]:.4f}')\n",
                "\n",
                "print(f'\\nüèÜ Best Model: {best_model_name}')\n",
                "print(f'   AUC-ROC:   {results[best_model_name][\"auc_roc\"]:.4f}')\n",
                "print(f'   Precision: {results[best_model_name][\"precision\"]:.4f}')\n",
                "print(f'   Recall:    {results[best_model_name][\"recall\"]:.4f}')\n",
                "\n",
                "print(f'\\nüìÅ Files Saved:')\n",
                "print('   - models/best_model.pkl')\n",
                "print('   - models/model_metrics.json')\n",
                "print('   - models/scaler.pkl')\n",
                "print('   - models/label_encoders.pkl')\n",
                "\n",
                "print(f'\\nüöÄ Next: Run the desktop app!')\n",
                "print('   python app_desktop.py')\n",
                "print('=' * 70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}